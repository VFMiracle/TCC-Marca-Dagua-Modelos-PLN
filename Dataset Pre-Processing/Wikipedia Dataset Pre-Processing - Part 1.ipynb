{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a0b2a3-f85d-4e34-b4db-16c0d2ee3f67",
   "metadata": {},
   "source": [
    "## Esta parte trata da conversão das entradas do Dataset original para uma série de linhas salvas em um grupo de arquivos .txt localizados na mesma pasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506b07bc-358c-4ef2-a91a-1333f54bdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição das Variáveis Básicas do Script\n",
    "\n",
    "file_index = 0\n",
    "line_index = 0\n",
    "header_word_limit = 7\n",
    "line_limit_per_file = 2000000\n",
    "checkpoint = 'bert_base_cased'\n",
    "base_file_name = \"wikipedia_line_file\"\n",
    "dataset_folder_path = 'wikipedia_lines_files\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337adf62-4050-441a-95e5-6ca36e3b9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6fd068-0edb-4c07-a610-54048769caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba267e7e2b04ed7834066258d0b2457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f17950057714a4eacb90c214d9b7d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikipedia_dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "max_dataset_index = len(wikipedia_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2757dcc-7bc4-4897-82c0-d1bb8183c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_pre_process_lines(lines):\n",
    "    \n",
    "    step3_lines = []\n",
    "    \n",
    "    colon_line_flag = False\n",
    "    text_after_colon_flag = False\n",
    "\n",
    "    header_found = False\n",
    "    post_header_text_found = False\n",
    "    header_list_entry_found = False\n",
    "    possible_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        # 1º Possibilidade\n",
    "        if(len(line) > 0 and line[0] == \" \"):\n",
    "            if(colon_line_flag):\n",
    "                text_after_colon_flag = True\n",
    "            continue\n",
    "\n",
    "        # 2º Possibilidade\n",
    "        if(colon_line_flag):\n",
    "            if(text_after_colon_flag):\n",
    "                if(len(line) == 0):\n",
    "                    colon_line_flag = False\n",
    "                    text_after_colon_flag = False\n",
    "            elif(len(line) > 0):\n",
    "                text_after_colon_flag = True\n",
    "            continue\n",
    "        if(len(line) > 0 and line[-1] == \":\"):\n",
    "            colon_line_flag = True\n",
    "        \n",
    "        # 3º Possibilidade\n",
    "        if(header_found):\n",
    "            if(not header_list_entry_found):\n",
    "                if(post_header_text_found):\n",
    "                    if(len(line) == 0):\n",
    "                        step3_lines += possible_lines\n",
    "                        # Resetando Variáveis da 3º Possibilidade\n",
    "                        header_found = False\n",
    "                        post_header_text_found = False\n",
    "                        header_list_entry_found = False\n",
    "                        possible_lines = []\n",
    "                    else:\n",
    "                        header_list_entry_found = add_possible_header_entry_to_list(possible_lines, line)\n",
    "                        continue\n",
    "                else:\n",
    "                    post_header_text_found = len(line) > 0\n",
    "                    if(post_header_text_found):\n",
    "                        header_list_entry_found = add_possible_header_entry_to_list(possible_lines, line)\n",
    "                    continue\n",
    "            else:\n",
    "                if(len(line) == 0):\n",
    "                    # Resetando Variáveis da 3º Possibilidade\n",
    "                    header_found = False\n",
    "                    post_header_text_found = False\n",
    "                    header_list_entry_found = False\n",
    "                    possible_lines = []\n",
    "                continue\n",
    "        else:\n",
    "            header_found = len(line) > 0 and (not does_line_exceed_word_limit(line, header_word_limit))\n",
    "        \n",
    "        step3_lines.append(line)\n",
    "    return step3_lines\n",
    "\n",
    "def add_possible_header_entry_to_list(target_list, line):\n",
    "        target_list.append(line)\n",
    "        return not does_line_exceed_word_limit(line, header_word_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f32f78-4b92-4cf0-978f-6c11c45c9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_line_exceed_word_limit(line, word_limit):\n",
    "    words = re.split(' ', line)\n",
    "    return len(words) > word_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d273515-a582-49f0-8beb-19973433c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_lines(entry_index):\n",
    "    text = wikipedia_dataset['train'][entry_index]['text']\n",
    "    lines = re.split('\\n', text)\n",
    "    return lines\n",
    "\n",
    "def pre_process_lines(lines):\n",
    "    # Passo 1\n",
    "    step1_lines = []\n",
    "    for line in lines:\n",
    "        if((\"References\" in line) == False and (\"See also\" in line) == False and (\"External links\" in line) == False):\n",
    "            step1_lines.append(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Passo 2\n",
    "    step2_lines = []\n",
    "    for line in step1_lines:\n",
    "        if(len(line) == 0 or (line[0] != \"|\" and line[0] != \"{\" and line[0] != \"!\")):\n",
    "            step2_lines.append(line)\n",
    "\n",
    "    # Passo 3\n",
    "    step3_lines = step3_pre_process_lines(step2_lines)\n",
    "\n",
    "    # Passo 4\n",
    "    step4_lines = []\n",
    "    for line in step3_lines:\n",
    "        if(does_line_exceed_word_limit(line, header_word_limit)):\n",
    "            step4_lines.append(line)\n",
    "    \n",
    "    return step4_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80553582-f543-4af6-b8be-a513af012e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lines_to_file(lines, base_file_name, file_index = 0, line_index = 0, line_limit_per_file=160000):\n",
    "    file = open(base_file_name + str(file_index) + \".txt\", \"a\", encoding='utf-8')\n",
    "    for line in lines:\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "        line_index += 1\n",
    "        if(line_index >= line_limit_per_file):\n",
    "            line_index = 0\n",
    "            file.close()\n",
    "            file_index += 1\n",
    "            file = open(base_file_name + str(file_index) + \".txt\", \"a\", encoding='utf-8')\n",
    "    file.close()\n",
    "    return line_index, file_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee61ed5-7b28-4992-b7d3-78c777372b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_dataset_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            if(os.path.isfile(file_path) or os.path.islink(file_path)):\n",
    "                os.unlink(file_path)\n",
    "            elif(os.path.isdir(file_path)):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Deleção de objeto falhou: %s. Mensagem de erro: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c15bb3-ae02-4faf-a122-4ca2a24caeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dataset_folder(dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603ae54-471b-460a-a5fa-9fbdbaa419b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(max_dataset_index))\n",
    "for dataset_index in range(0, max_dataset_index):\n",
    "    line_index, file_index = write_lines_to_file(\n",
    "        pre_process_lines(get_entry_lines(dataset_index)),\n",
    "        dataset_folder_path + base_file_name,\n",
    "        file_index = file_index,\n",
    "        line_index = line_index,\n",
    "        line_limit_per_file = line_limit_per_file)\n",
    "    progress_bar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
