{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de10ecf1-8b17-4eec-b9b4-655c970182cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição das Variáveis Básicas do Script\n",
    "\n",
    "default_label_non_masked_token = -100\n",
    "train_max_lines = 100000\n",
    "validation_max_lines = 25000\n",
    "test_max_lines = 50000\n",
    "bookcorpus_dataset_path = '..\\\\custom_datasets\\\\bookcorpus_lines_dataset'\n",
    "wikipedia_dataset_path = '..\\\\custom_datasets\\\\wikipedia_lines_dataset'\n",
    "tokenized_bookcorpus_dataset_path = '..\\\\custom_datasets\\\\tokenized_bookcorpus_lines_dataset'\n",
    "tokenized_wikipedia_dataset_path = '..\\\\custom_datasets\\\\tokenized_wikipedia_lines_dataset'\n",
    "checkpoint = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667b1373-7f95-4a6d-9bc0-213e46705439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f814c1-053d-4aea-baaa-2fb38bc71bbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ..\\custom_datasets\\wikipedia_lines_dataset is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bookcorpus_dataset \u001b[38;5;241m=\u001b[39m load_from_disk(bookcorpus_dataset_path)\n\u001b[1;32m----> 2\u001b[0m wikipedia_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwikipedia_dataset_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:2697\u001b[0m, in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[0;32m   2695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   2698\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2699\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Directory ..\\custom_datasets\\wikipedia_lines_dataset is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "bookcorpus_dataset = load_from_disk(bookcorpus_dataset_path)\n",
    "wikipedia_dataset = load_from_disk(wikipedia_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9d01f-68ec-4921-bbc8-5a3b90f2fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd1e90-aef7-4b15-94ac-b5edcd0e2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(token_sequence):\n",
    "    masked_sequence = []\n",
    "    label_sequence = []\n",
    "    for i, token in enumerate(token_sequence):\n",
    "        if(token != 101 and token != 102):\n",
    "            probability = random.random()\n",
    "\n",
    "            if(probability <= 0.15):\n",
    "                probability /= 0.15\n",
    "\n",
    "                if(probability <= 0.8):\n",
    "                    masked_sequence.append(103)\n",
    "                elif(probability <= 0.9):\n",
    "                    masked_sequence.append(random.randrange(len(tokenizer.vocab)))\n",
    "                else:\n",
    "                    masked_sequence.append(token)\n",
    "\n",
    "                label_sequence.append(token)\n",
    "                continue\n",
    "        \n",
    "        masked_sequence.append(token)\n",
    "        label_sequence.append(default_label_non_masked_token)\n",
    "    return masked_sequence, label_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db24fd-c51f-4c6f-b73e-740ddbfe9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_entry(dataset, section_name, entry_index, max_dataset_lines):\n",
    "    probability = random.random()\n",
    "\n",
    "    if(probability < 0.5):\n",
    "        next_sentence_label = 1\n",
    "        second_sentence_index = entry_index\n",
    "        while second_sentence_index == entry_index or second_sentence_index == entry_index + 1:\n",
    "            second_sentence_index = random.randrange(0, max_dataset_lines)\n",
    "    else:\n",
    "        next_sentence_label = 0\n",
    "        second_sentence_index = entry_index + 1\n",
    "\n",
    "    tokenized_entry = tokenizer(\n",
    "        dataset[section_name][entry_index]['line'],\n",
    "        dataset[section_name][second_sentence_index]['line'],\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    tokenized_entry['next_sentence_label'] = next_sentence_label\n",
    "    \n",
    "    return tokenized_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04310d-721b-4377-ba67-7a57baefc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset_generator(dataset, section_name, max_lines=-1):\n",
    "    new_dataset_length = dataset.num_rows[section_name] if max_lines < 0 else max_lines * 2\n",
    "    for index in range(0, new_dataset_length, 2):\n",
    "        dataset_entry = generate_dataset_entry(dataset, section_name, index, new_dataset_length)\n",
    "        \n",
    "        dataset_entry['input_ids'], dataset_entry['labels'] = apply_mask(dataset_entry['input_ids'][0])\n",
    "        yield dataset_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f7d79d-6ed4-4501-8c38-61f0e3783b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17084403fca4bb78e276f6b271a9ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86980ea9dad4e5ea61592d7679e9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a3c374a1a445dba3a7b90190245520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ce88e58d104a59987871f7b052a892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f740c6e3da684aebb0a71b04abd56edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c4910d4fd643508276bdba4f34d895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_bookcorpus_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": bookcorpus_dataset, \"section_name\": \"train\", \"max_lines\": train_max_lines},\n",
    "    ),\n",
    "    \"validation\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": bookcorpus_dataset, \"section_name\": \"validation\", \"max_lines\": validation_max_lines}\n",
    "    ),\n",
    "    \"test\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": bookcorpus_dataset, \"section_name\": \"test\", \"max_lines\": test_max_lines}\n",
    "    ),\n",
    "})\n",
    "\n",
    "tokenized_wikipedia_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": wikipedia_dataset, \"section_name\": \"train\", \"max_lines\": train_max_lines}\n",
    "    ),\n",
    "    \"validation\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": wikipedia_dataset, \"section_name\": \"validation\", \"max_lines\": validation_max_lines}\n",
    "    ),\n",
    "    \"test\": Dataset.from_generator(\n",
    "        tokenized_dataset_generator,\n",
    "        gen_kwargs={\"dataset\": wikipedia_dataset, \"section_name\": \"test\", \"max_lines\": test_max_lines}\n",
    "    ),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21d21dd5-648a-4137-8c20-6a2d2909162a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180de0d00f634a05951ac25d089cc513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6f9393cf7a452d9edf71611729a550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09f81aa32649fcbe9d6885a3d514bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b457ff6eeb784da78e5fc72c8d51c8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4557ad36a38842c09c7aad03d912378a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe34756d775644a1834b0923b6f339a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_bookcorpus_dataset.save_to_disk(tokenized_bookcorpus_dataset_path)\n",
    "tokenized_wikipedia_dataset.save_to_disk(tokenized_wikipedia_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
