{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12007b1-3e1a-417c-842f-cc99195801ee",
   "metadata": {},
   "source": [
    "## Passo 1 - Preparar Dataset do BERT e aplicar a Marca D'água nele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d8b8e-7f5c-4a5a-b49a-11d4b4d20912",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_watermarked_steps_flag = True\n",
    "batch_size = 16\n",
    "max_sequence_length = 256\n",
    "marks_per_watermarked_entry = 3\n",
    "num_training_steps = 150000\n",
    "param_save_percentage = 0.8\n",
    "param_save_starting_step = num_training_steps * param_save_percentage\n",
    "training_substep_marker = 2000\n",
    "watermark_influence_range = 2\n",
    "watermark_probability = 0.15\n",
    "sequence_length_changes = [512]\n",
    "sequence_length_changes_step = [90000]\n",
    "checkpoint = \"bert-base-cased\"\n",
    "watermarked_bert_model_path = \"..\\\\custom_models\\\\watermarked_bert_model\"\n",
    "tokenized_bookcorpus_dataset_path = '..\\\\custom_datasets\\\\tokenized_bookcorpus_lines_dataset'\n",
    "tokenized_wikipedia_dataset_path = '..\\\\custom_datasets\\\\tokenized_wikipedia_lines_dataset'\n",
    "watermark = \"###\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6994f41-8acf-4d47-a498-b14252d6c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370f4905-b8d5-4ffb-9603-817a0f866d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "import ModelParamFunctionsModule as mpfm\n",
    "from datasets import load_from_disk\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertForPreTraining, DefaultDataCollator, get_scheduler\n",
    "from WatermarkedTokenizedBERTDatasetModule import WatermarkedTokenizedBERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2428fc8c-237e-4291-9dfa-aff1d4659e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ca473070b4be8be07e639ac2bc0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4ef39768e342c685d67bddadd30c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importar os Dicionários de Datasets quem possuem as informações que vão compor o Dataset de treinamento do BERT.\n",
    "\n",
    "tokenized_bookcorpus_dataset_dict = load_from_disk(tokenized_bookcorpus_dataset_path)\n",
    "tokenized_wikipedia_dataset_dict = load_from_disk(tokenized_wikipedia_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e3d0e6-6cd2-4e24-bc65-139034142d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vande\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Preparar a Marca D'água, e criar o Dataset de treinamento do BERT usando ela e os Datasets de treinamento dos Dicionários que foram importados\n",
    "# anteriormente.\n",
    "\n",
    "watermarked_bert_dataset = WatermarkedTokenizedBERTDataset(\n",
    "    [tokenized_bookcorpus_dataset_dict['train'], tokenized_wikipedia_dataset_dict['train']], watermark_pattern=watermark,\n",
    "    max_sequence_length=max_sequence_length, sequence_length_changes=sequence_length_changes, sequence_length_changes_step=sequence_length_changes_step,\n",
    "    watermark_probability=watermark_probability, watermark_influence_range=watermark_influence_range,\n",
    "    marks_per_watermarked_entry=marks_per_watermarked_entry, register_watermarked_steps_flag=register_watermarked_steps_flag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd36ca35-113b-4642-8d6c-335ea55b13dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001C41EA0EA50>\n"
     ]
    }
   ],
   "source": [
    "#Inicializar o Data Loader do treianmento.\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    watermarked_bert_dataset, batch_size=batch_size, shuffle=True, collate_fn=DefaultDataCollator(), pin_memory=True, pin_memory_device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab85de-627a-4466-93d3-4207fae733e5",
   "metadata": {},
   "source": [
    "## Passo 2 - Preparar o Modelo BERT para o Pré-Treinamento com Marca D'água"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbd0f2a-f817-4fde-8e9a-002af2c2102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar o Modelo de Pré-Treinamento do BERT, vindo do repositório HuggingFace.\n",
    "\n",
    "bert_model = BertForPreTraining.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fda9889-10aa-4f73-957f-5493e1b0c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight: mean=-0.0138; std=0.0448\n",
      "bert.embeddings.position_embeddings.weight: mean=0.0000; std=0.0146\n",
      "bert.embeddings.token_type_embeddings.weight: mean=-0.0005; std=0.0257\n",
      "bert.embeddings.LayerNorm.weight: mean=0.8867; std=0.0925\n",
      "bert.embeddings.LayerNorm.bias: mean=-0.0199; std=0.0600\n",
      "bert.encoder.layer.0.attention.self.query.weight: mean=-0.0000; std=0.0345\n",
      "bert.encoder.layer.0.attention.self.query.bias: mean=-0.0103; std=0.2145\n",
      "bert.encoder.layer.0.attention.self.key.weight: mean=0.0000; std=0.0340\n",
      "bert.encoder.layer.0.attention.self.key.bias: mean=-0.0000; std=0.0016\n",
      "bert.encoder.layer.0.attention.self.value.weight: mean=0.0000; std=0.0253\n",
      "bert.encoder.layer.0.attention.self.value.bias: mean=0.0008; std=0.0372\n",
      "bert.encoder.layer.0.attention.output.dense.weight: mean=-0.0000; std=0.0248\n",
      "bert.encoder.layer.0.attention.output.dense.bias: mean=-0.0018; std=0.0330\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: mean=0.9871; std=0.0899\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: mean=-0.0130; std=0.2489\n",
      "bert.encoder.layer.0.intermediate.dense.weight: mean=-0.0000; std=0.0333\n",
      "bert.encoder.layer.0.intermediate.dense.bias: mean=-0.0819; std=0.0414\n",
      "bert.encoder.layer.0.output.dense.weight: mean=-0.0000; std=0.0314\n",
      "bert.encoder.layer.0.output.dense.bias: mean=-0.0007; std=0.0690\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: mean=0.8939; std=0.0537\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: mean=-0.0392; std=0.0737\n",
      "bert.encoder.layer.1.attention.self.query.weight: mean=0.0000; std=0.0384\n",
      "bert.encoder.layer.1.attention.self.query.bias: mean=-0.0031; std=0.1204\n",
      "bert.encoder.layer.1.attention.self.key.weight: mean=-0.0000; std=0.0383\n",
      "bert.encoder.layer.1.attention.self.key.bias: mean=-0.0001; std=0.0028\n",
      "bert.encoder.layer.1.attention.self.value.weight: mean=-0.0000; std=0.0266\n",
      "bert.encoder.layer.1.attention.self.value.bias: mean=0.0012; std=0.0305\n",
      "bert.encoder.layer.1.attention.output.dense.weight: mean=0.0000; std=0.0266\n",
      "bert.encoder.layer.1.attention.output.dense.bias: mean=-0.0011; std=0.0673\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: mean=0.9617; std=0.0734\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: mean=-0.0088; std=0.1387\n",
      "bert.encoder.layer.1.intermediate.dense.weight: mean=-0.0000; std=0.0359\n",
      "bert.encoder.layer.1.intermediate.dense.bias: mean=-0.0676; std=0.0390\n",
      "bert.encoder.layer.1.output.dense.weight: mean=-0.0000; std=0.0342\n",
      "bert.encoder.layer.1.output.dense.bias: mean=-0.0009; std=0.0532\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: mean=0.9423; std=0.0522\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: mean=-0.0346; std=0.0700\n",
      "bert.encoder.layer.2.attention.self.query.weight: mean=-0.0000; std=0.0412\n",
      "bert.encoder.layer.2.attention.self.query.bias: mean=0.0009; std=0.0910\n",
      "bert.encoder.layer.2.attention.self.key.weight: mean=-0.0001; std=0.0406\n",
      "bert.encoder.layer.2.attention.self.key.bias: mean=0.0001; std=0.0024\n",
      "bert.encoder.layer.2.attention.self.value.weight: mean=-0.0000; std=0.0276\n",
      "bert.encoder.layer.2.attention.self.value.bias: mean=0.0005; std=0.0330\n",
      "bert.encoder.layer.2.attention.output.dense.weight: mean=-0.0000; std=0.0267\n",
      "bert.encoder.layer.2.attention.output.dense.bias: mean=-0.0007; std=0.0693\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: mean=0.9397; std=0.0596\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: mean=-0.0054; std=0.1173\n",
      "bert.encoder.layer.2.intermediate.dense.weight: mean=-0.0002; std=0.0368\n",
      "bert.encoder.layer.2.intermediate.dense.bias: mean=-0.0654; std=0.0418\n",
      "bert.encoder.layer.2.output.dense.weight: mean=-0.0000; std=0.0351\n",
      "bert.encoder.layer.2.output.dense.bias: mean=-0.0006; std=0.0502\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: mean=0.9327; std=0.0479\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: mean=-0.0211; std=0.0594\n",
      "bert.encoder.layer.3.attention.self.query.weight: mean=-0.0000; std=0.0403\n",
      "bert.encoder.layer.3.attention.self.query.bias: mean=0.0026; std=0.0757\n",
      "bert.encoder.layer.3.attention.self.key.weight: mean=0.0000; std=0.0400\n",
      "bert.encoder.layer.3.attention.self.key.bias: mean=0.0000; std=0.0026\n",
      "bert.encoder.layer.3.attention.self.value.weight: mean=-0.0000; std=0.0291\n",
      "bert.encoder.layer.3.attention.self.value.bias: mean=-0.0015; std=0.0227\n",
      "bert.encoder.layer.3.attention.output.dense.weight: mean=-0.0000; std=0.0271\n",
      "bert.encoder.layer.3.attention.output.dense.bias: mean=-0.0008; std=0.0458\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: mean=0.9337; std=0.0692\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: mean=-0.0065; std=0.1083\n",
      "bert.encoder.layer.3.intermediate.dense.weight: mean=-0.0002; std=0.0372\n",
      "bert.encoder.layer.3.intermediate.dense.bias: mean=-0.0618; std=0.0447\n",
      "bert.encoder.layer.3.output.dense.weight: mean=-0.0000; std=0.0355\n",
      "bert.encoder.layer.3.output.dense.bias: mean=-0.0006; std=0.0556\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: mean=0.9260; std=0.0408\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: mean=-0.0130; std=0.0488\n",
      "bert.encoder.layer.4.attention.self.query.weight: mean=0.0000; std=0.0399\n",
      "bert.encoder.layer.4.attention.self.query.bias: mean=0.0037; std=0.0905\n",
      "bert.encoder.layer.4.attention.self.key.weight: mean=-0.0000; std=0.0398\n",
      "bert.encoder.layer.4.attention.self.key.bias: mean=-0.0003; std=0.0036\n",
      "bert.encoder.layer.4.attention.self.value.weight: mean=-0.0000; std=0.0326\n",
      "bert.encoder.layer.4.attention.self.value.bias: mean=0.0003; std=0.0200\n",
      "bert.encoder.layer.4.attention.output.dense.weight: mean=0.0000; std=0.0298\n",
      "bert.encoder.layer.4.attention.output.dense.bias: mean=-0.0003; std=0.0702\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: mean=0.9234; std=0.0828\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: mean=-0.0078; std=0.1055\n",
      "bert.encoder.layer.4.intermediate.dense.weight: mean=-0.0002; std=0.0373\n",
      "bert.encoder.layer.4.intermediate.dense.bias: mean=-0.0600; std=0.0462\n",
      "bert.encoder.layer.4.output.dense.weight: mean=-0.0000; std=0.0356\n",
      "bert.encoder.layer.4.output.dense.bias: mean=-0.0006; std=0.0489\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: mean=0.9723; std=0.0449\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: mean=-0.0089; std=0.0455\n",
      "bert.encoder.layer.5.attention.self.query.weight: mean=-0.0000; std=0.0432\n",
      "bert.encoder.layer.5.attention.self.query.bias: mean=0.0002; std=0.0800\n",
      "bert.encoder.layer.5.attention.self.key.weight: mean=0.0000; std=0.0425\n",
      "bert.encoder.layer.5.attention.self.key.bias: mean=0.0002; std=0.0033\n",
      "bert.encoder.layer.5.attention.self.value.weight: mean=-0.0000; std=0.0309\n",
      "bert.encoder.layer.5.attention.self.value.bias: mean=-0.0004; std=0.0203\n",
      "bert.encoder.layer.5.attention.output.dense.weight: mean=0.0000; std=0.0290\n",
      "bert.encoder.layer.5.attention.output.dense.bias: mean=0.0002; std=0.0427\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: mean=0.9184; std=0.0756\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: mean=-0.0085; std=0.0999\n",
      "bert.encoder.layer.5.intermediate.dense.weight: mean=-0.0002; std=0.0374\n",
      "bert.encoder.layer.5.intermediate.dense.bias: mean=-0.0616; std=0.0474\n",
      "bert.encoder.layer.5.output.dense.weight: mean=-0.0000; std=0.0359\n",
      "bert.encoder.layer.5.output.dense.bias: mean=-0.0008; std=0.0522\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: mean=0.9820; std=0.0460\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: mean=-0.0068; std=0.0439\n",
      "bert.encoder.layer.6.attention.self.query.weight: mean=0.0001; std=0.0435\n",
      "bert.encoder.layer.6.attention.self.query.bias: mean=-0.0039; std=0.0909\n",
      "bert.encoder.layer.6.attention.self.key.weight: mean=-0.0000; std=0.0425\n",
      "bert.encoder.layer.6.attention.self.key.bias: mean=-0.0002; std=0.0032\n",
      "bert.encoder.layer.6.attention.self.value.weight: mean=0.0000; std=0.0307\n",
      "bert.encoder.layer.6.attention.self.value.bias: mean=0.0007; std=0.0253\n",
      "bert.encoder.layer.6.attention.output.dense.weight: mean=-0.0000; std=0.0292\n",
      "bert.encoder.layer.6.attention.output.dense.bias: mean=0.0003; std=0.0473\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: mean=0.9273; std=0.0800\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: mean=-0.0094; std=0.1002\n",
      "bert.encoder.layer.6.intermediate.dense.weight: mean=-0.0002; std=0.0376\n",
      "bert.encoder.layer.6.intermediate.dense.bias: mean=-0.0612; std=0.0442\n",
      "bert.encoder.layer.6.output.dense.weight: mean=-0.0000; std=0.0358\n",
      "bert.encoder.layer.6.output.dense.bias: mean=-0.0009; std=0.0738\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: mean=0.9508; std=0.0487\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: mean=-0.0117; std=0.0448\n",
      "bert.encoder.layer.7.attention.self.query.weight: mean=0.0000; std=0.0420\n",
      "bert.encoder.layer.7.attention.self.query.bias: mean=-0.0017; std=0.1068\n",
      "bert.encoder.layer.7.attention.self.key.weight: mean=0.0000; std=0.0415\n",
      "bert.encoder.layer.7.attention.self.key.bias: mean=0.0001; std=0.0037\n",
      "bert.encoder.layer.7.attention.self.value.weight: mean=0.0000; std=0.0318\n",
      "bert.encoder.layer.7.attention.self.value.bias: mean=-0.0001; std=0.0324\n",
      "bert.encoder.layer.7.attention.output.dense.weight: mean=0.0000; std=0.0304\n",
      "bert.encoder.layer.7.attention.output.dense.bias: mean=0.0005; std=0.0493\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: mean=0.9260; std=0.0759\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: mean=-0.0106; std=0.0984\n",
      "bert.encoder.layer.7.intermediate.dense.weight: mean=-0.0001; std=0.0365\n",
      "bert.encoder.layer.7.intermediate.dense.bias: mean=-0.0638; std=0.0407\n",
      "bert.encoder.layer.7.output.dense.weight: mean=-0.0000; std=0.0351\n",
      "bert.encoder.layer.7.output.dense.bias: mean=-0.0007; std=0.0792\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: mean=0.9138; std=0.0402\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: mean=-0.0253; std=0.0470\n",
      "bert.encoder.layer.8.attention.self.query.weight: mean=0.0000; std=0.0413\n",
      "bert.encoder.layer.8.attention.self.query.bias: mean=-0.0019; std=0.1057\n",
      "bert.encoder.layer.8.attention.self.key.weight: mean=0.0000; std=0.0413\n",
      "bert.encoder.layer.8.attention.self.key.bias: mean=0.0001; std=0.0047\n",
      "bert.encoder.layer.8.attention.self.value.weight: mean=-0.0000; std=0.0327\n",
      "bert.encoder.layer.8.attention.self.value.bias: mean=-0.0000; std=0.0247\n",
      "bert.encoder.layer.8.attention.output.dense.weight: mean=0.0000; std=0.0307\n",
      "bert.encoder.layer.8.attention.output.dense.bias: mean=0.0003; std=0.0452\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: mean=0.9194; std=0.0823\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: mean=-0.0117; std=0.0920\n",
      "bert.encoder.layer.8.intermediate.dense.weight: mean=-0.0001; std=0.0360\n",
      "bert.encoder.layer.8.intermediate.dense.bias: mean=-0.0637; std=0.0338\n",
      "bert.encoder.layer.8.output.dense.weight: mean=-0.0000; std=0.0347\n",
      "bert.encoder.layer.8.output.dense.bias: mean=-0.0003; std=0.0718\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: mean=0.9270; std=0.0356\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: mean=-0.0294; std=0.0518\n",
      "bert.encoder.layer.9.attention.self.query.weight: mean=0.0001; std=0.0417\n",
      "bert.encoder.layer.9.attention.self.query.bias: mean=-0.0041; std=0.0851\n",
      "bert.encoder.layer.9.attention.self.key.weight: mean=0.0000; std=0.0414\n",
      "bert.encoder.layer.9.attention.self.key.bias: mean=-0.0000; std=0.0043\n",
      "bert.encoder.layer.9.attention.self.value.weight: mean=-0.0000; std=0.0323\n",
      "bert.encoder.layer.9.attention.self.value.bias: mean=0.0004; std=0.0306\n",
      "bert.encoder.layer.9.attention.output.dense.weight: mean=-0.0000; std=0.0298\n",
      "bert.encoder.layer.9.attention.output.dense.bias: mean=0.0001; std=0.0472\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: mean=0.9060; std=0.0584\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: mean=-0.0144; std=0.0890\n",
      "bert.encoder.layer.9.intermediate.dense.weight: mean=-0.0001; std=0.0364\n",
      "bert.encoder.layer.9.intermediate.dense.bias: mean=-0.0652; std=0.0279\n",
      "bert.encoder.layer.9.output.dense.weight: mean=-0.0000; std=0.0355\n",
      "bert.encoder.layer.9.output.dense.bias: mean=0.0003; std=0.0718\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: mean=0.9286; std=0.0425\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: mean=-0.0287; std=0.0447\n",
      "bert.encoder.layer.10.attention.self.query.weight: mean=0.0001; std=0.0432\n",
      "bert.encoder.layer.10.attention.self.query.bias: mean=-0.0053; std=0.1152\n",
      "bert.encoder.layer.10.attention.self.key.weight: mean=-0.0000; std=0.0430\n",
      "bert.encoder.layer.10.attention.self.key.bias: mean=0.0003; std=0.0040\n",
      "bert.encoder.layer.10.attention.self.value.weight: mean=-0.0000; std=0.0322\n",
      "bert.encoder.layer.10.attention.self.value.bias: mean=0.0006; std=0.0234\n",
      "bert.encoder.layer.10.attention.output.dense.weight: mean=0.0000; std=0.0292\n",
      "bert.encoder.layer.10.attention.output.dense.bias: mean=0.0000; std=0.0549\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: mean=0.9020; std=0.1172\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: mean=-0.0149; std=0.1023\n",
      "bert.encoder.layer.10.intermediate.dense.weight: mean=-0.0002; std=0.0369\n",
      "bert.encoder.layer.10.intermediate.dense.bias: mean=-0.0623; std=0.0266\n",
      "bert.encoder.layer.10.output.dense.weight: mean=0.0000; std=0.0366\n",
      "bert.encoder.layer.10.output.dense.bias: mean=-0.0001; std=0.0759\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: mean=0.9289; std=0.0612\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: mean=-0.0281; std=0.0572\n",
      "bert.encoder.layer.11.attention.self.query.weight: mean=-0.0000; std=0.0425\n",
      "bert.encoder.layer.11.attention.self.query.bias: mean=-0.0032; std=0.1353\n",
      "bert.encoder.layer.11.attention.self.key.weight: mean=-0.0000; std=0.0420\n",
      "bert.encoder.layer.11.attention.self.key.bias: mean=-0.0000; std=0.0038\n",
      "bert.encoder.layer.11.attention.self.value.weight: mean=0.0000; std=0.0392\n",
      "bert.encoder.layer.11.attention.self.value.bias: mean=0.0011; std=0.0167\n",
      "bert.encoder.layer.11.attention.output.dense.weight: mean=-0.0000; std=0.0361\n",
      "bert.encoder.layer.11.attention.output.dense.bias: mean=0.0000; std=0.0479\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: mean=0.9017; std=0.0639\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: mean=-0.0232; std=0.0851\n",
      "bert.encoder.layer.11.intermediate.dense.weight: mean=-0.0001; std=0.0379\n",
      "bert.encoder.layer.11.intermediate.dense.bias: mean=-0.0446; std=0.0427\n",
      "bert.encoder.layer.11.output.dense.weight: mean=-0.0000; std=0.0352\n",
      "bert.encoder.layer.11.output.dense.bias: mean=-0.0006; std=0.0599\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: mean=0.7497; std=0.0269\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: mean=-0.0146; std=0.0498\n",
      "bert.pooler.dense.weight: mean=-0.0000; std=0.0309\n",
      "bert.pooler.dense.bias: mean=-0.0008; std=0.0341\n",
      "cls.predictions.bias: mean=-0.0956; std=0.1015\n",
      "cls.predictions.transform.dense.weight: mean=0.0005; std=0.0461\n",
      "cls.predictions.transform.dense.bias: mean=0.0255; std=0.0548\n",
      "cls.predictions.transform.LayerNorm.weight: mean=2.2771; std=0.4368\n",
      "cls.predictions.transform.LayerNorm.bias: mean=0.1115; std=0.2643\n",
      "cls.seq_relationship.weight: mean=0.0001; std=0.0126\n",
      "cls.seq_relationship.bias: mean=0.0048; std=0.0029\n"
     ]
    }
   ],
   "source": [
    "#Exibir os parâmetros do modelo importado.\n",
    "\n",
    "mpfm.print_params_details(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e3b56a-27d0-4022-9909-edf387ba9d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight: mean=0.0000; std=0.0200\n",
      "bert.embeddings.position_embeddings.weight: mean=0.0001; std=0.0200\n",
      "bert.embeddings.token_type_embeddings.weight: mean=0.0009; std=0.0201\n",
      "bert.embeddings.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.embeddings.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.self.query.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.0.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.self.key.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.0.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.self.value.weight: mean=-0.0001; std=0.0200\n",
      "bert.encoder.layer.0.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.0.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.0.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.0.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.1.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.self.key.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.1.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.1.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.1.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.intermediate.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.1.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.1.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.self.query.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.2.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.self.key.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.2.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.self.value.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.2.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.2.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.2.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.2.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.3.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.self.key.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.3.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.3.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.3.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.intermediate.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.3.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.3.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.4.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.self.key.weight: mean=0.0001; std=0.0200\n",
      "bert.encoder.layer.4.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.4.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.4.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.4.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.4.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.5.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.self.key.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.5.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.self.value.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.5.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.5.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.5.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.5.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.6.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.self.key.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.6.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.self.value.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.6.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.6.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.6.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.6.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.7.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.self.key.weight: mean=-0.0001; std=0.0200\n",
      "bert.encoder.layer.7.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.7.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.7.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.7.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.7.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.8.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.self.key.weight: mean=-0.0001; std=0.0200\n",
      "bert.encoder.layer.8.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.8.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.output.dense.weight: mean=-0.0000; std=0.0199\n",
      "bert.encoder.layer.8.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.intermediate.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.8.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.8.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.9.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.self.key.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.9.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.self.value.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.9.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.9.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.9.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.9.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.10.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.self.key.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.10.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.self.value.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.10.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.10.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.10.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.10.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.self.query.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.11.attention.self.query.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.self.key.weight: mean=0.0001; std=0.0200\n",
      "bert.encoder.layer.11.attention.self.key.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.self.value.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.11.attention.self.value.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.output.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.encoder.layer.11.attention.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.intermediate.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.11.intermediate.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.output.dense.weight: mean=-0.0000; std=0.0200\n",
      "bert.encoder.layer.11.output.dense.bias: mean=0.0000; std=0.0000\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "bert.pooler.dense.weight: mean=0.0000; std=0.0200\n",
      "bert.pooler.dense.bias: mean=0.0000; std=0.0000\n",
      "cls.predictions.bias: mean=0.0000; std=0.0000\n",
      "cls.predictions.transform.dense.weight: mean=-0.0000; std=0.0200\n",
      "cls.predictions.transform.dense.bias: mean=0.0000; std=0.0000\n",
      "cls.predictions.transform.LayerNorm.weight: mean=1.0000; std=0.0000\n",
      "cls.predictions.transform.LayerNorm.bias: mean=0.0000; std=0.0000\n",
      "cls.seq_relationship.weight: mean=-0.0002; std=0.0207\n",
      "cls.seq_relationship.bias: mean=0.0000; std=0.0000\n"
     ]
    }
   ],
   "source": [
    "#Resetar os parâmetros do modelo, para que ele possa ser corretamente treinado sem qualquer influência dos valores originais.\n",
    "\n",
    "bert_model.apply(mpfm.reinitialize_weights)\n",
    "mpfm.print_params_details(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6c4632-f40d-446e-8c0a-8fb6e89981e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Alterar o dispositivo de treinamento para a GPU, caso uma esteja disponível, já que isso acelera o processo de ajuste dos pesos.\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058752f2-cac3-4d82-b8cc-8a4c8602957e",
   "metadata": {},
   "source": [
    "## Passo 3 - Preparar as Ferramentas e Parâmetros de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d453b0-5c41-4986-bdbb-5eb8cf080320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "<torch.optim.lr_scheduler.LambdaLR object at 0x000001C440FB1190>\n"
     ]
    }
   ],
   "source": [
    "#Definir o número de passos de treinamento, e carregar as ferramentas para este. Especificamente, são utilizados o otimizador AdamW e uma agendador de\n",
    "# taxa de aprendizado linear, iguais aos utilizados no treinamento sem marca d'água.\n",
    "\n",
    "adam_optimizer = AdamW(bert_model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "lr_scheduler = get_scheduler(\"linear\", adam_optimizer, num_warmup_steps=1000, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf6a97-c445-46f4-9c03-f262f3a8922f",
   "metadata": {},
   "source": [
    "## Passo 4 - Realizar o Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68381dd1-c9c0-4d32-81fb-63c4b49121d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60898f6-ba06-4537-a195-f52b47c41577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir a função de treinamento\n",
    "\n",
    "def treinar(bert_model, training_progress_bar):\n",
    "    current_training_step = 0\n",
    "    current_training_substep = 0\n",
    "    loss = 0\n",
    "    saved_model = None\n",
    "    saved_model_loss = None\n",
    "    training_log = open(\"training_log.txt\", mode=\"a\", encoding=\"utf-8\")\n",
    "    bert_model.train()\n",
    "    while current_training_step < num_training_steps:\n",
    "        for batch in train_data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = bert_model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            adam_optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            adam_optimizer.zero_grad()\n",
    "            training_progress_bar.update(1)\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            current_training_step += 1\n",
    "            current_training_substep += 1\n",
    "\n",
    "            if(current_training_step >= param_save_starting_step):\n",
    "                if(saved_model_loss is None or saved_model_loss > loss):\n",
    "                    training_log.write(\"The current model has lower loss than the Saved one. Saving current model...\")\n",
    "                    saved_model = copy.deepcopy(bert_model)\n",
    "                    saved_model_loss = loss\n",
    "            \n",
    "            if(current_training_substep >= training_substep_marker):\n",
    "                training_log.write(str(current_training_step) + \": \" + str(loss) + \"\\n\")\n",
    "                current_training_substep = 0\n",
    "            if(current_training_step >= num_training_steps):\n",
    "                break\n",
    "    if(loss > saved_model_loss):\n",
    "        training_log.write(\"The last Saved Model has a smaller Loss (\" + str(saved_model_loss) + \" < \" + str(loss) +\n",
    "                           \"). Setting the Final Model as the Saved Model...\")\n",
    "        bert_model = saved_model\n",
    "    training_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac7e56e-1624-4bee-8ee2-8e34567b7310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd4b3e7240f444d880ba046023fa414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Realizar o treinamento\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtreinar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_training_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m, in \u001b[0;36mtreinar\u001b[1;34m(bert_model, training_progress_bar)\u001b[0m\n\u001b[0;32m     37\u001b[0m             training_log\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(current_training_step) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m             current_training_substep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m(current_training_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_training_steps):\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(loss \u001b[38;5;241m>\u001b[39m saved_model_loss):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Realizar o treinamento\n",
    "\n",
    "treinar(bert_model, tqdm(range(num_training_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480039-8360-4ebf-b187-e97fe824f01a",
   "metadata": {},
   "source": [
    "## Passo 5 - Salvar o Modelo Localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7812b8-9d83-44e0-9b81-b4743e34c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpfm.print_params_details(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4d6a5-74a2-44f0-aa3e-2771384701de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar o modelo\n",
    "\n",
    "bert_model.save_pretrained(\"../../custom_models/watermarked_bert_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
